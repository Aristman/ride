# Инструкция по настройке Ollama в Ride плагине

## Что добавлено

В Ride плагин добавлена поддержка локальной LLM модели **Ollama** для чата. Теперь вы можете использовать локальные модели вместо облачных сервисов.

## Установка и настройка Ollama

### 1. Установите Ollama

```bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Или следуйте инструкциям на https://ollama.ai/download
```

### 2. Установите модель

```bash
# Для чата (рекомендуется)
ollama pull llama3:8b

# Для эмбеддингов (используется в RAG системе)
ollama pull nomic-embed-text

# Другие доступные модели
ollama pull llama3:70b
ollama pull codellama:7b
ollama pull mistral:7b
ollama pull qwen:7b
ollama pull gemma:7b
```

### 3. Запустите Ollama сервер

```bash
ollama serve
```

Сервер будет доступен по адресу `http://localhost:11434`

## Настройка в Ride плагине

### 1. Откройте настройки

- Откройте IntelliJ IDEA
- Перейдите в `File → Settings → Tools → Ride`
- Выберите вкладку `LlmConfig`

### 2. Выберите провайдера

- В выпадающем списке `Provider / Model` выберите `Ollama (Local)`

### 3. Настройте параметры

- **Base URL**: `http://localhost:11434` (по умолчанию)
- **Model**: Выберите одну из установленных моделей:
  - `Llama 3 8B` - рекомендуется для большинства задач
  - `Llama 3 70B` - более мощная модель
  - `CodeLlama 7B` - специализирована для кода
  - `Mistral 7B` - сбалансированная модель
  - `Qwen 7B` - многоязычная модель
  - `Gemma 7B` - модель от Google

### 4. Сохраните настройки

Нажмите `Apply` и `OK` для сохранения настроек.

## Использование

После настройки вы можете:

1. **Общаться с AI** через Ride чат (`View → Tool Windows → Ride Chat`)
2. **Использовать RAG обогащение** - эмбеддинги автоматически будут использовать `nomic-embed-text`
3. **Переключаться между моделями** в настройках без перезапуска IDE

## Доступные модели

### Для чата (рекомендованные):
- **Llama 3 8B** - оптимальный баланс скорости и качества
- **Llama 3 8B Instruct** - улучшенная версия для инструкций
- **Mistral 7B** - хорошая производительность
- **Qwen 7B** - поддерживает множество языков, включая русский

### Для эмбеддингов (RAG система - используется автоматически):
- **Nomic Embed Text** - 768 измерений, хорошая точность
- **All MiniLM** - 384 измерений, быстрее но менее точная

### Проверенные рабочие модели:
- **Qwen 7B** - ✅ протестирована и работает корректно
- **Llama 3 8B** - ✅ популярная модель с хорошей производительностью
- **Mistral 7B** - ✅ быстрая и надежная модель

**Примечание:** Установлены только те модели, которые были успешно протестированы. Некоторые модели из других источников могут быть недоступны или работать некорректно.

## Устранение неполадок

### Ollama недоступен
- Убедитесь что Ollama сервер запущен: `ollama serve`
- Проверьте доступность: `curl http://localhost:11434/api/tags`
- Проверьте что модель установлена: `ollama list`

### Медленные ответы
- Попробуйте более легкую модель (8B вместо 70B)
- Убедитесь что у вас достаточно RAM
- Проверьте загрузку CPU

### Ошибки в чате
- Проверьте логи в IntelliJ IDEA (`Help → Show Log in Explorer`)
- Убедитесь что модель корректно установлена
- Перезапустите Ollama сервер

## Преимущества локальных моделей

- **Конфиденциальность** - данные не покидают ваш компьютер
- **Бесплатно** - нет стоимости за использование API
- **Офлайн работа** - работает без подключения к интернету
- **Быстрый отклик** - нет задержек сети
- **Кастомизация** - можно fine-tune модели под свои задачи